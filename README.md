# LLM-Backed Log Summarizer and Alert Responder

An intelligent log monitoring system that uses Large Language Models (LLMs) to analyze system logs, detect anomalies, and execute automated remediation actions via Model Context Protocol (MCP).

---

## ðŸŽ¯ Quick Demo Checklist

**For a successful 5-minute demonstration:**

1. âœ… **Setup** (one-time):

   ```bash
   pip3 install -r requirements-demo.txt --break-system-packages
   python3 database.py
   export OPENROUTER_API_KEY="your-key-here"   # Required â€” get key from https://openrouter.ai/keys
   ```

   (Use `requirements.txt` instead of `requirements-demo.txt` only if your environment can install `python-seccomp`; otherwise the demo file avoids that error on some WSL setups.)
2. âœ… **Run Evaluation** (populates database with test data):

   ```bash
   python3 cli_interface.py evaluate-dataset test_datasets/sample_logs.csv 20
   ```
3. âœ… **Test Queries** (shows LLM analysis):

   ```bash
   python3 cli_interface.py query "summarize last hour errors"
   python3 cli_interface.py query "show open incidents"
   python3 cli_interface.py query "should I restart apache"
   ```
4. âœ… **Live Monitoring** (optional, impressive demo):

   - Terminal 1: `bash monitor.sh`
   - Terminal 2: `echo "ERROR: Test error" >> server.log`

**See [Demonstration Guide](#-demonstration-guide-wsl) for detailed steps.**

---

## ðŸ“‹ Description

This project implements an end-to-end log monitoring and incident response system that combines the power of Large Language Models with practical automation. The system continuously monitors log files (such as web server logs, syslog, or application logs), detects anomalies using pattern matching, analyzes incidents using LLM technology, and can automatically execute remediation actions through secure Bash scripts.

**What makes this system special:**

- **Intelligent Analysis**: Uses LLM to understand log context and provide meaningful summaries
- **Safe Automation**: All actions go through approval gates and validation via Model Context Protocol
- **Query Interface**: Ask questions about logs in natural language ("What errors occurred in the last hour?")
- **Automated Alert Rules**: System suggests and generates monitoring rules based on observed patterns
- **Complete Audit Trail**: Every action is logged for security and compliance

---

## ðŸŽ¯ Background

System administrators and DevOps engineers face the challenge of monitoring vast amounts of log data generated by modern systems. Traditional log analysis tools require complex rule configuration and struggle with:

- Multiple log formats across different services
- Understanding context and relationships between log entries
- Rapidly responding to incidents
- Scaling manual analysis

This project addresses these challenges by:

1. Using LLMs to understand log semantics and context
2. Providing natural language interfaces for querying logs
3. Automating remediation actions through safe, validated scripts
4. Offering real-time monitoring with intelligent anomaly detection

**Research Foundation:**
The implementation draws from research in log analysis (DeepLog, LogAnomaly), LLM agent systems (ReAct, Toolformer), and safety frameworks (Model Context Protocol). While the current implementation uses simplified pattern-based detection, the architecture supports future enhancements such as vector embeddings and advanced template mining.

---

## ðŸ“ Scope

### What This System Does:

âœ… **Log Ingestion**: Continuously monitors log files in real-time using streaming (similar to `tail -f`)
âœ… **Anomaly Detection**: Detects errors, critical events, failures, and timeouts using configurable patterns
âœ… **LLM Analysis**: Uses OpenRouter API to analyze log context and generate intelligent summaries
âœ… **Query Interface**: Allows natural language queries about historical logs ("summarize last hour's errors")
âœ… **Time Windows**: Query logs by time ranges (last hour, last 24 hours, custom periods)
âœ… **Alert Rule Generation**: Automatically suggests and generates Bash scripts for alert rules
âœ… **Automated Remediation**: Executes Bash scripts to fix issues (restart services, clear cache, etc.)
âœ… **Safety Controls**: Approval gates, dry-run mode, and comprehensive audit logging
âœ… **Metrics Tracking**: Calculates Mean Time To Detect (MTTD) and Mean Time To Recover (MTTR)
âœ… **Evaluation Framework**: Measures system performance (latency, accuracy, false positives)

### What This System Does NOT Do:

âŒ **Vector Embeddings**: Not implemented (architecture supports as future enhancement)
âŒ **Template Mining**: No automatic log template extraction algorithms (Drain/Spell)
âŒ **RAG Pipeline**: Uses direct LLM calls, not retrieval-augmented generation
âŒ **Machine Learning Models**: Uses pattern-based detection, not trained ML models
âŒ **Real-Time Vector Search**: Uses SQLite relational queries, not semantic similarity search

---

## âœ¨ Features and Capabilities

### Core Features

1. **Real-Time Log Monitoring**

   - Streams logs using `tail -F` (Linux) or similar methods
   - Detects anomalies using configurable keyword patterns
   - Stores all incidents in SQLite database
2. **LLM-Powered Analysis**

   - Uses OpenRouter API (supports GPT-3.5-turbo, GPT-4, and other models)
   - Generates intelligent summaries of log incidents
   - Suggests appropriate remediation actions
   - Returns structured JSON responses
3. **Query Interface**

   - Natural language queries: "summarize last hour's errors"
   - Time-window queries: "what happened in the last 24 hours?"
   - Pattern searches: "find suspicious authentication events"
   - Service-specific queries: "should I restart apache?"
4. **Alert Rule Generation**

   - Analyzes log patterns to suggest alert rules
   - Generates regex patterns automatically
   - Suggests thresholds based on frequency
   - Creates Bash scripts for alert monitoring
5. **MCP Framework (Model Context Protocol)**

   - Validates all tool invocations
   - Ensures actions match defined schemas
   - Provides audit trail for all operations
   - Enables safe automation
6. **Safety Features**

   - **Approval Gates**: High-risk actions require human approval
   - **Dry-Run Mode**: Test actions without actual execution
   - **Audit Logging**: Complete trail of all actions
   - **Risk Levels**: Actions classified as HIGH/MEDIUM/LOW risk
7. **Database Persistence**

   - SQLite database stores all incidents, actions, and logs
   - Time-window queries for historical analysis
   - Pattern-based search capabilities
   - Relational indexing for fast queries
8. **Metrics and Evaluation**

   - MTTD (Mean Time To Detect): How quickly incidents are identified
   - MTTR (Mean Time To Recover): How quickly incidents are resolved
   - Latency metrics: LLM API response times
   - Detection accuracy: Precision, recall, false positives

### Technical Capabilities

- **Language**: Python 3.7+, Bash scripting
- **Database**: SQLite (lightweight, file-based)
- **API Integration**: OpenRouter API (multi-model LLM access)
- **Protocol**: Model Context Protocol (MCP) for tool safety
- **Platform**: Linux/Unix (tested on WSL Windows)

---

## ðŸš€ Installation & Setup

### Prerequisites

Before starting, ensure you have:

1. **Python 3.7 or higher**

   ```bash
   python3 --version  # Should show 3.7 or higher
   ```
2. **Linux/Unix environment**

   - Tested on WSL (Windows Subsystem for Linux)
   - Ubuntu/Debian recommended
   - macOS/Linux also supported
3. **OpenRouter API Key** (optional but recommended)

   - Sign up at https://openrouter.ai
   - Get your API key from the dashboard
   - Free tier available for testing
4. **Internet Connection**

   - Required for LLM API calls
   - OpenRouter API must be accessible

### Step-by-Step Installation

#### Step 1: Navigate to Project Directory

If using WSL:

```bash
cd /mnt/c/Users/Admin/Desktop/LLM\ PROJECT\ ZIP/llm-responder
```

Or if you cloned/downloaded the project:

```bash
cd /path/to/llm-responder
```

#### Step 2: Install Python Dependencies

```bash
# Install required packages
pip3 install -r requirements.txt --break-system-packages

# Or if you have a virtual environment:
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
pip install -r requirements.txt
```

**Required packages:**

- `requests` - For API calls
- `sqlite3` - Database (usually included with Python)

#### Step 3: Initialize Database

```bash
# Create and initialize SQLite database
python3 database.py
```

This creates `incidents.db` with the necessary tables for:

- Storing log entries
- Recording incidents
- Tracking actions
- Storing metrics

#### Step 4: Set API Key (Required)

The project uses the **environment variable** for the OpenRouter API key. No config file is used.

```bash
# Set for current session
export OPENROUTER_API_KEY="your-api-key-here"

# Get your key from: https://openrouter.ai/keys

# Optional: add to ~/.bashrc for persistence (WSL/Linux)
echo 'export OPENROUTER_API_KEY="your-api-key-here"' >> ~/.bashrc
source ~/.bashrc
```

#### Step 5: Verify Installation

```bash
# Check Python version
python3 --version

# Verify database was created
ls -lh incidents.db

# Test basic functionality
python3 cli_interface.py query "summarize last hour errors"
```

If you see a response (even if it says "No errors found"), the installation is successful!

---

## ðŸ“– Usage Guide

### Query Interface

Ask questions about your logs in natural language. **Supported query patterns:**

```bash
# Summarize recent errors (supports "last hour", "last 24 hours", "last X hours")
python3 cli_interface.py query "summarize last hour errors"
python3 cli_interface.py query "summarize last 24 hours errors"

# Check service health (apache or nginx)
python3 cli_interface.py query "should I restart apache"
python3 cli_interface.py query "should I restart nginx"

# View open incidents
python3 cli_interface.py query "show open incidents"

# Query suspicious authentication events
python3 cli_interface.py query "what suspicious auth events occurred"
```

**Note:** The query interface uses pattern matching. Queries must contain keywords like "summarize errors", "restart", "open incidents", or "suspicious auth" to be recognized.

### Alert Rule Generation

Generate monitoring rules automatically:

```bash
python3 cli_interface.py suggest-rules
```

This analyzes log patterns and suggests alert rules with:

- Regex patterns for detection
- Recommended thresholds
- Suggested actions

### Live Monitoring

Monitor a log file in real-time:

```bash
# Monitor default log file (server.log)
# Note: server.log will be created automatically if it doesn't exist
bash monitor.sh

# Monitor custom log file
bash monitor.sh /var/log/application.log
```

**Before monitoring, you can add test log entries:**

```bash
# Create sample log entries for testing
echo "INFO: System started successfully" > server.log
echo "ERROR: Apache failed to start. Port 80 is in use." >> server.log
echo "CRITICAL: Database connection timeout after 30 seconds" >> server.log

# Then start monitoring
bash monitor.sh
```

The monitor will:

- Stream new log entries (using `tail -F`)
- Detect anomalies using keyword patterns (ERROR, CRITICAL, Failed, Timeout)
- Call LLM for analysis
- Suggest remediation actions
- Store incidents in database

**To stop monitoring:** Press `Ctrl+C`

**To test in real-time:** Open a second terminal and add log entries:

```bash
# In second terminal, add new log entries
echo "ERROR: Test error message" >> server.log
# The monitor will detect and process it immediately
```

### View Metrics

Check system performance metrics:

```bash
# View operational metrics (MTTD, MTTR)
python3 metrics.py

# View summary of all metrics
python3 cli_interface.py metrics
```

---

## ðŸ“Š Running Evaluation

The evaluation framework measures system performance on test datasets. This is essential for understanding detection accuracy, latency, and summarization quality.

### Quick Evaluation

Run a basic evaluation with default test data:

```bash
python3 cli_interface.py evaluate
```

This evaluates:

- Detection accuracy (precision, recall, F1-score)
- LLM latency (average, min, max response times)
- Summarization quality
- Operational metrics (MTTD, MTTR)

### Comprehensive Dataset Evaluation

Evaluate on the full test dataset with detailed metrics:

```bash
# Run evaluation on test dataset
python3 cli_interface.py evaluate-dataset test_datasets/sample_logs.csv

# Limit number of logs for faster testing (optional)
python3 cli_interface.py evaluate-dataset test_datasets/sample_logs.csv 20
```

**What this evaluates:**

- **Detection Metrics:**

  - True Positives: Correctly detected anomalies
  - False Positives: Normal logs incorrectly flagged
  - False Negatives: Anomalies that were missed
  - Precision: Percentage of detected anomalies that are actually anomalies
  - Recall: Percentage of actual anomalies that were detected
  - F1-Score: Harmonic mean of precision and recall
- **Latency Metrics:**

  - Average LLM API response time
  - Minimum and maximum response times
  - Response time distribution
- **Summarization Quality:**

  - Summary coverage (percentage of logs with summaries)
  - Average summary length
  - Keyword coverage
  - Informativeness score

### Viewing Evaluation Results

After running evaluation, results are saved to JSON files:

```bash
# View latest evaluation results
cat evaluation_results_*.json | python3 -m json.tool

# View formatted results (first 50 lines)
cat evaluation_results_*.json | python3 -m json.tool | head -50

# View specific metrics
python3 -c "import json; f=open('evaluation_results_*.json'); d=json.load(f); print(f'Precision: {d[\"precision\"]}%'); print(f'Recall: {d[\"recall\"]}%'); print(f'F1-Score: {d[\"f1_score\"]}%')"
```

### Understanding Evaluation Results

**Expected Results (with realistic dataset):**

- **Precision:** ~50-70% (pattern-based detection has limitations)
- **Recall:** ~50-70% (some anomalies won't match keywords)
- **F1-Score:** ~50-70% (balanced metric)
- **False Positives:** Some normal logs may trigger alerts
- **False Negatives:** Some anomalies may be missed
- **Average Latency:** 0.5-2.0 seconds (depends on API)

**Why not 100%?**

- Pattern-based detection uses simple keyword matching
- Real-world logs have varied formats and terminology
- Some anomalies don't use standard keywords (WARNING, FATAL, etc.)
- Some normal logs contain error-related words in non-error contexts

This is **expected and realistic** - perfect scores would indicate the test dataset is too simple.

### Evaluation Report Generation

Generate a detailed markdown report:

```bash
# The evaluation automatically generates EVALUATION_REPORT.md
cat EVALUATION_REPORT.md

# Or view the LaTeX report (if generated)
cat evaluation_report.tex
```

### Troubleshooting Evaluation

**Issue: "No such file or directory: test_datasets/sample_logs.csv"**

```bash
# Verify dataset exists
ls -la test_datasets/sample_logs.csv

# If missing, the dataset should be included in the project
# Check that you're in the correct directory
pwd
```

**Issue: "API key not found" or HTTP 401**

```bash
# Set API key in your environment (required)
export OPENROUTER_API_KEY="your-key-here"

# Get a key from https://openrouter.ai/keys if you don't have one

# Verify it's set
echo $OPENROUTER_API_KEY
```

**Issue: "Evaluation takes too long"**

```bash
# Use limit parameter to test on fewer logs
python3 cli_interface.py evaluate-dataset test_datasets/sample_logs.csv 10
```

**Issue: "Database locked"**

```bash
# Close any other processes using the database
# Or delete and recreate
rm incidents.db
python3 database.py
```

---

## ðŸŽ¬ Demonstration Guide (WSL)

This guide follows a **clear demo flow**: initialize once, start the monitor (the core pipeline), feed it logs so the database is populated, then show all other features with real data. Use **two terminals**: one for the monitor, one for commands and feeding logs.

---

### Phase 1: Initialization (one-time)

#### Step 1: Navigate to Project Directory

```bash
cd /mnt/c/Users/Admin/Desktop/LLM\ PROJECT\ ZIP/llm-responder
```

#### Step 2: Set API Key (required)

```bash
export OPENROUTER_API_KEY="your-key-here"
```

Get a key at https://openrouter.ai/keys if needed.

#### Step 3: Verify Setup

```bash
python3 --version
ls -la
```

#### Step 4: Initialize Database

```bash
python3 database.py
```

You should see: `Database initialized successfully!`

---

### Phase 2: Start the Monitor (core pipeline)

**Open a second terminal** for the rest of the demo. Keep **Terminal 1** for the monitor and use **Terminal 2** for feeding logs and running CLI commands.

#### Step 5: Start the Log Monitor (Terminal 1)

In **Terminal 1** (same directory, API key set):

```bash
cd /mnt/c/Users/Admin/Desktop/LLM\ PROJECT\ ZIP/llm-responder
export OPENROUTER_API_KEY="your-key-here"
bash monitor.sh
```

Leave this running. You should see: `Log Monitor - Real-Time Streaming Active` and `Target: server.log`. The monitor runs in **demo mode** (clean, formatted output: one block per incident with ANOMALY, SUMMARY, ACTION, MCP, STATUS). It watches `server.log` and will detect anomalies (ERROR, CRITICAL, Failed, Timeout), call the LLM, and record incidents.

---

### Phase 3: Feed Logs (populate the database)

#### Step 6: Feed Logs (Terminal 2)

In **Terminal 2**, navigate to the project and add log lines so the monitor detects them:

**Option A â€“ Quick test (a few lines):**

```bash
cd /mnt/c/Users/Admin/Desktop/LLM\ PROJECT\ ZIP/llm-responder
echo "ERROR: Apache failed to start. Port 80 is in use." >> server.log
echo "CRITICAL: Database connection timeout after 30 seconds" >> server.log
echo "ERROR: Failed to connect to Redis server at 127.0.0.1:6379" >> server.log
```

**Option B â€“ Full realistic log (project includes a long `server.log`):**

```bash
cd /mnt/c/Users/Admin/Desktop/LLM\ PROJECT\ ZIP/llm-responder
cp server.log server.log.backup
> server.log
cat server.log.backup >> server.log
```

**Watch Terminal 1** while lines are appended. You should see for each anomaly:

- ðŸš¨ [ANOMALY DETECTED] + the log line
- [LLM SUMMARY] (OpenRouter analysis)
- [PROPOSED ACTION] (e.g. RESTART_APACHE, ESCALATE)
- [MCP GATEWAY] / incident handling

Wait for a few anomalies to be processed (or for the full log to finish) before moving on. You can stop the monitor later with `Ctrl+C` in Terminal 1, or leave it running.

---

### Phase 4: Show Other Features (Terminal 2)

All commands below run in **Terminal 2**. Because the monitor just wrote incidents to the database, queries and suggest-rules will return **real data**.

#### Step 7: Test Query Interface (Natural Language)

```bash
python3 cli_interface.py query "summarize last hour errors"
python3 cli_interface.py query "show open incidents"
python3 cli_interface.py query "should I restart apache"
python3 cli_interface.py query "what suspicious auth events occurred"
```

You should see actual summaries and incident lists (not "no incidents").

#### Step 8: Generate Alert Rules

```bash
python3 cli_interface.py suggest-rules
```

Suggested rules are based on the patterns from the incidents the monitor just stored.

#### Step 9: View Metrics

```bash
python3 metrics.py
```

Shows MTTD, MTTR, incident counts, and related metrics.

#### Step 10: Check Database (optional)

```bash
sqlite3 incidents.db "SELECT incident_id, detected_at, status, LEFT(summary,50) FROM incidents;"
sqlite3 incidents.db "SELECT COUNT(*) as total_logs, SUM(is_anomaly) as anomalies FROM logs;"
```

#### Step 11: View Audit Log (optional)

```bash
cat mcp_audit.log | tail -20
```

#### Step 12: Stop the Monitor (when done with live part)

In **Terminal 1**, press `Ctrl+C` to stop `monitor.sh`.

---

### Phase 5: Evaluation (optional â€“ ties to paper Results section)

Run the dataset evaluation to show precision, recall, F1, and latency from the report:

```bash
python3 cli_interface.py evaluate-dataset test_datasets/sample_logs.csv 20
```

View the latest evaluation JSON (use the newest file to avoid "Extra data" when multiple files exist):

```bash
python3 -m json.tool $(ls -t evaluation_results_*.json | head -1) | head -80
```

Or print key metrics:

```bash
python3 -c "
import json, glob
files = glob.glob('evaluation_results_*.json')
if files:
    with open(max(files)) as f:
        d = json.load(f)
    de = d.get('dataset_evaluation', {})
    print('Precision:', de.get('precision'), '%')
    print('Recall:', de.get('recall'), '%')
    print('F1-Score:', de.get('f1_score'), '%')
    print('Avg latency:', de.get('latency', {}).get('average'), 's')
"
```

---

### Other Optional Steps

- **Config:** `cat config.json | python3 -m json.tool`
- **Project structure:** `tree -L 2 -I '__pycache__|*.pyc' 2>/dev/null || find . -maxdepth 2 -type f \( -name "*.py" -o -name "*.sh" \) | head -25`
- **Alert rule generator:** `python3 alert_rule_generator.py`
- **Generated alert scripts:** `ls -la *.sh; cat alert_rule_*.sh 2>/dev/null | head -30`

---

### Quick Demo Sequence (summary)

1. **Terminal 2:** Init (navigate, `export OPENROUTER_API_KEY`, `python3 database.py`).
2. **Terminal 1:** `bash monitor.sh` (leave running).
3. **Terminal 2:** Feed logs (Option A or B above).
4. **Terminal 2:** Queries â†’ suggest-rules â†’ metrics (and optionally DB, audit log, evaluation).
5. **Terminal 1:** `Ctrl+C` when done.

### Troubleshooting Commands

```bash
# Check if API key is set
echo $OPENROUTER_API_KEY

# Check Python dependencies
pip3 list | grep -E "requests|sqlite3"

# Verify database exists
ls -lh incidents.db

# Check for errors in recent runs
tail -50 mcp_audit.log 2>/dev/null || echo "No audit log found"
```

---

## ðŸŽ“ Key Features

### System Architecture

- Real-time log ingestion and pattern-based anomaly detection
- LLM-powered analysis with OpenRouter API
- MCP (Model Context Protocol) for safe tool invocation
- SQLite database for persistent storage
- Approval gates and dry-run mode for safety

### Research Foundation

This implementation draws from research in:

- Log analysis (DeepLog, LogAnomaly, LogBERT)
- LLM agent systems (ReAct, Toolformer, AutoGen)
- Safety frameworks (Model Context Protocol)

The architecture prioritizes simplicity and production-readiness while maintaining research grounding. Advanced techniques (vector embeddings, template mining, RAG) are identified as future enhancements.

---

## ðŸ“š Additional Resources

- **LLM_BACKED_LOG_SUMMARIZER.pdf** - Literature review and research foundation
- **Project Code** - All source files are documented with comments

---

## ðŸ¤ Questions During Demo?

**Q: Why use pattern-based detection instead of ML?**
A: Simplicity and reliability. Pattern-based detection is production-ready and easy to maintain. ML-based detection (like DeepLog) can be added as future enhancement.

**Q: Where are vector embeddings?**
A: The architecture supports vector embeddings, but the current implementation uses SQLite for faster deployment. Vector embeddings (SBERT/Faiss) are identified as future enhancement.

**Q: How does this compare to commercial SIEM tools?**
A: This system focuses on LLM-powered analysis and natural language interfaces, which many SIEM tools lack. It's designed to complement existing tools by adding intelligent analysis.

**Q: Is this production-ready?**
A: The core functionality is production-ready. Additional features (vector embeddings, template mining) can be added incrementally. The safety framework (MCP, approval gates) ensures safe operation.

---

## ðŸ“‹ Syslog-style feed for monitor.sh

Use a **second terminal** while `bash monitor.sh` runs in the first. Copy the block below (or parts of it) and append to `server.log` so the monitor detects anomalies (lines with ERROR, CRITICAL, Failed, Timeout) and processes them.

```bash
# From project root, in a second terminal:
cat >> server.log << 'FEED'
Feb 13 09:00:01 host1 systemd[1]: Started Daily apt download activities.
Feb 13 09:00:02 host1 cron[1023]: (root) CMD (test -x /usr/sbin/anacron || ( cd / && run-parts --report /etc/cron.daily ))
Feb 13 09:00:05 host1 apache2[4521]: AH00094: Command line: '/usr/sbin/apache2 -D FOREGROUND'
Feb 13 09:00:08 host1 kernel: [12345.678] audit: type=1326 audit(1707800408.123:456): auid=1000 uid=1000 gid=1000 ses=2 pid=7890 comm="nginx" exe="/usr/sbin/nginx"
Feb 13 09:00:10 host1 sshd[4522]: Accepted publickey for deploy from 10.0.0.2 port 54322 ssh2
Feb 13 09:00:12 host1 postfix/qmgr[1200]: A1B2C3D4E5: from=<noreply@example.com>, size=1024, nrcpt=1
Feb 13 09:00:15 host1 apache2[4521]: 192.168.1.10 - - [13/Feb/2025:09:00:15 +0000] "GET /api/health HTTP/1.1" 200 42
Feb 13 09:00:18 host1 redis-server[3001]: 1:M 13 Feb 2025 09:00:18.123 * DB loaded from disk
Feb 13 09:00:20 host1 mysqld[2100]: 2025-02-13T09:00:20.123456Z 0 [Note] InnoDB: Buffer pool(s) load completed
Feb 13 09:00:22 host1 app-backend[7001]: INFO: Request processed successfully - GET /api/status 200
Feb 13 09:00:25 host1 nginx[4001]: 10.0.0.5 - - [13/Feb/2025:09:00:25 +0000] "POST /api/login HTTP/1.1" 200 128
Feb 13 09:00:28 host1 systemd[1]: Starting Docker Application Container Engine...
Feb 13 09:00:30 host1 docker[5000]: time="2025-02-13T09:00:30.123456789Z" level=info msg="Container started" container=app-web id=abc123
Feb 13 09:00:32 host1 app-backend[7001]: INFO: Cache hit for key session_xyz - latency 1ms
Feb 13 09:00:35 host1 kernel: [12380.000] device eth0 entered promiscuous mode
Feb 13 09:00:38 host1 apache2[4521]: ERROR: Apache failed to start. Port 80 is in use.
Feb 13 09:00:40 host1 sshd[4522]: Accepted password for admin from 192.168.1.100 port 22 ssh2
Feb 13 09:00:42 host1 postfix/smtp[3200]: A1B2C3D4E5: to=<user@example.com>, relay=mail.example.com[10.0.0.1]:25, delay=2.1, status=sent
Feb 13 09:00:45 host1 app-backend[7001]: INFO: Database connection pool initialized (max_connections=20)
Feb 13 09:00:48 host1 nginx[4001]: 10.0.0.6 - - [13/Feb/2025:09:00:48 +0000] "GET /dashboard HTTP/1.1" 200 4096
Feb 13 09:00:50 host1 redis-server[3001]: 2:C 13 Feb 2025 09:00:50.456 * Ready to accept connections
Feb 13 09:00:52 host1 mysqld[2100]: 2025-02-13T09:00:52.789012Z 0 [Note] Event Scheduler: scheduler thread started
Feb 13 09:00:55 host1 cron[1024]: (www-data) CMD (/usr/local/bin/backup-db.sh)
Feb 13 09:00:58 host1 app-backend[7001]: CRITICAL: Database connection timeout after 30 seconds
Feb 13 09:01:00 host1 kernel: [12405.000] audit: type=1101 audit(1707800460.456:457): pid=7891 uid=0 auid=1000 ses=2 msg='op=login id=7891 exe="/usr/sbin/sshd" hostname=10.0.0.2 addr=10.0.0.2 terminal=ssh res=success'
Feb 13 09:01:02 host1 docker[5000]: time="2025-02-13T09:01:02Z" level=info msg="Network default bridge created"
Feb 13 09:01:05 host1 apache2[4521]: 192.168.1.11 - - [13/Feb/2025:09:01:05 +0000] "GET /api/metrics HTTP/1.1" 200 512
Feb 13 09:01:08 host1 app-backend[7001]: INFO: User authentication successful for user=operator source=10.0.0.5
Feb 13 09:01:10 host1 redis-server[3001]: 3:M 13 Feb 2025 09:01:10.789 * Background saving started by pid 3100
Feb 13 09:01:12 host1 nginx[4001]: ERROR: Failed to connect to Redis server at 127.0.0.1:6379
Feb 13 09:01:15 host1 postfix/cleanup[3201]: B2C3D4E5F6: message-id=<abc@example.com>
Feb 13 09:01:18 host1 systemd[1]: Started Session 42 of user deploy.
Feb 13 09:01:20 host1 mysqld[2100]: 2025-02-13T09:01:20.123456Z 0 [Note] InnoDB: Starting crash recovery
Feb 13 09:01:22 host1 app-backend[7001]: INFO: Request processed successfully - POST /api/orders 201
Feb 13 09:01:25 host1 kernel: [12430.000] IPv6: ADDRCONF(NETDEV_CHANGE): eth0: link becomes ready
Feb 13 09:01:28 host1 docker[5000]: time="2025-02-13T09:01:28Z" level=info msg="Container app-worker started" container=app-worker id=def456
Feb 13 09:01:30 host1 apache2[4521]: AH00124: Request received from client: 192.168.1.12
Feb 13 09:01:32 host1 sshd[4522]: Received disconnect from 10.0.0.2 port 54322:11: disconnected by user
Feb 13 09:01:35 host1 app-backend[7001]: ERROR: Permission denied: cannot write to /var/log/app/application.log
Feb 13 09:01:38 host1 redis-server[3001]: 4:S 13 Feb 2025 09:01:38.012 * DB saved on disk
Feb 13 09:01:40 host1 nginx[4001]: 10.0.0.7 - - [13/Feb/2025:09:01:40 +0000] "GET /static/js/app.js HTTP/1.1" 200 8192
Feb 13 09:01:42 host1 postfix/qmgr[1200]: B2C3D4E5F6: removed
Feb 13 09:01:45 host1 cron[1025]: (root) CMD (/usr/lib/apt/apt.systemd.daily install)
Feb 13 09:01:48 host1 app-backend[7001]: INFO: Background job started: sync_inventory job_id=9001
Feb 13 09:01:50 host1 kernel: [12455.000] docker0: port 1(veth123) entered forwarding state
Feb 13 09:01:52 host1 mysqld[2100]: 2025-02-13T09:01:52.456789Z 0 [Note] /usr/sbin/mysqld: ready for connections.
Feb 13 09:01:55 host1 systemd[1]: Stopping Docker Application Container Engine...
Feb 13 09:01:58 host1 docker[5000]: time="2025-02-13T09:01:58Z" level=warning msg="Failed to stop container app-cache: context deadline exceeded"
Feb 13 09:02:00 host1 apache2[4521]: 192.168.1.13 - - [13/Feb/2025:09:02:00 +0000] "GET /api/health HTTP/1.1" 200 42
Feb 13 09:02:02 host1 app-backend[7001]: CRITICAL: Disk space below 5% threshold on /var
Feb 13 09:02:05 host1 redis-server[3001]: 5:M 13 Feb 2025 09:02:05.234 * Background saving terminated with success
Feb 13 09:02:08 host1 nginx[4001]: 10.0.0.8 - - [13/Feb/2025:09:02:08 +0000] "POST /api/webhook HTTP/1.1" 202 64
Feb 13 09:02:10 host1 sshd[4522]: Accepted publickey for git from 10.0.0.3 port 54324 ssh2
Feb 13 09:02:12 host1 postfix/smtp[3202]: C3D4E5F6G7: to=<ops@example.com>, relay=mail.example.com[10.0.0.1]:25, delay=1.5, status=sent
Feb 13 09:02:15 host1 app-backend[7001]: INFO: Cache hit for key config_v2 - latency 0ms
Feb 13 09:02:18 host1 kernel: [12473.000] audit: type=1300 audit(1707800538.789:458): arch=40000028 syscall=44 success=yes exit=0 a0=4 a1=7fff0000 a2=0 a3=0 items=1 ppid=7000 pid=7001 auid=1000 uid=33 gid=33 euid=33 suid=33 fsuid=33 egid=33 sgid=33 fsgid=33 tty=(none) ses=2 comm="app-backend" exe="/usr/bin/python3"
Feb 13 09:02:20 host1 docker[5000]: time="2025-02-13T09:02:20Z" level=info msg="Container app-cache stopped"
Feb 13 09:02:22 host1 mysqld[2100]: 2025-02-13T09:02:22.123456Z 0 [Warning] InnoDB: Retrying truncate of table mysql/innodb_table_stats
Feb 13 09:02:25 host1 apache2[4521]: AH00060: child 4522 seems to have been replaced by child 4523
Feb 13 09:02:28 host1 app-backend[7001]: ERROR: LDAP connection timeout after 15 seconds - host=ldap.internal
Feb 13 09:02:30 host1 nginx[4001]: 10.0.0.9 - - [13/Feb/2025:09:02:30 +0000] "GET /api/users HTTP/1.1" 200 2048
Feb 13 09:02:32 host1 redis-server[3001]: 6:C 13 Feb 2025 09:02:32.567 * The server is now ready to accept connections on port 6379
Feb 13 09:02:35 host1 systemd[1]: Started Run anacron jobs.
Feb 13 09:02:38 host1 cron[1026]: (root) CMD ([ -x /usr/sbin/update-notifier-common ] && /usr/sbin/update-notifier-common)
Feb 13 09:02:40 host1 postfix/cleanup[3203]: D4E5F6G7H8: message-id=<def@example.com>
Feb 13 09:02:42 host1 app-backend[7001]: INFO: Request processed successfully - GET /api/orders 200
Feb 13 09:02:45 host1 kernel: [12490.000] docker0: port 1(veth123) entered disabled state
Feb 13 09:02:48 host1 docker[5000]: time="2025-02-13T09:02:48Z" level=info msg="Container app-web restarted" container=app-web id=abc123
Feb 13 09:02:50 host1 apache2[4521]: 192.168.1.14 - - [13/Feb/2025:09:02:50 +0000] "GET /favicon.ico HTTP/1.1" 404 196
Feb 13 09:02:52 host1 sshd[4522]: pam_unix(sshd:session): session opened for user deploy(uid=1001) by (uid=0)
Feb 13 09:02:55 host1 app-backend[7001]: CRITICAL: Out of memory: process worker-3 killed by OOM
Feb 13 09:02:58 host1 mysqld[2100]: 2025-02-13T09:02:58.789012Z 0 [Note] InnoDB: Flush completed
Feb 13 09:03:00 host1 nginx[4001]: 10.0.0.10 - - [13/Feb/2025:09:03:00 +0000] "GET /api/health HTTP/1.1" 503 128
Feb 13 09:03:02 host1 redis-server[3001]: 7:M 13 Feb 2025 09:03:02.890 * Synchronization with replica 10.0.0.20:6379 succeeded
Feb 13 09:03:05 host1 postfix/qmgr[1200]: D4E5F6G7H8: from=<alerts@example.com>, size=512, nrcpt=1
Feb 13 09:03:08 host1 app-backend[7001]: INFO: Health check passed - all dependencies reachable
Feb 13 09:03:10 host1 systemd[1]: Started certbot.timer - Run certbot twice daily
Feb 13 09:03:12 host1 kernel: [12507.000] device veth456 entered promiscuous mode
Feb 13 09:03:15 host1 apache2[4521]: ERROR: Configuration check failed: Syntax error in /etc/apache2/sites-enabled/app.conf line 12
Feb 13 09:03:18 host1 docker[5000]: time="2025-02-13T09:03:18Z" level=error msg="Failed to pull image app:v2: connection timeout"
Feb 13 09:03:20 host1 app-backend[7001]: INFO: Request processed successfully - GET /api/status 200
Feb 13 09:03:22 host1 nginx[4001]: 10.0.0.11 - - [13/Feb/2025:09:03:22 +0000] "GET / HTTP/1.1" 200 1024
Feb 13 09:03:25 host1 mysqld[2100]: 2025-02-13T09:03:25.123456Z 0 [ERROR] InnoDB: Unable to lock ./ibdata1, errno: 11
Feb 13 09:03:28 host1 redis-server[3001]: 8:S 13 Feb 2025 09:03:28.123 * Connection from 10.0.0.20 port 6379 accepted
Feb 13 09:03:30 host1 postfix/smtp[3204]: D4E5F6G7H8: to=<ops@example.com>, relay=mail.example.com[10.0.0.1]:25, delay=3.2, status=sent
Feb 13 09:03:32 host1 app-backend[7001]: INFO: Session created for user admin ttl=3600
Feb 13 09:03:35 host1 cron[1027]: (backup) CMD (/usr/local/bin/backup-files.sh /var/data)
Feb 13 09:03:38 host1 kernel: [12523.000] audit: type=1100 audit(1707800618.012:459): pid=8000 uid=0 auid=1000 ses=2 msg='op=login id=8000 exe="/usr/bin/sudo" hostname=host1 addr=192.168.1.1 terminal=/dev/pts/0 res=success'
Feb 13 09:03:40 host1 apache2[4521]: 192.168.1.15 - - [13/Feb/2025:09:03:40 +0000] "GET /api/version HTTP/1.1" 200 24
Feb 13 09:03:42 host1 sshd[4522]: Accepted publickey for root from 192.168.1.1 port 22 ssh2
Feb 13 09:03:45 host1 app-backend[7001]: ERROR: Failed to connect to Redis server at 127.0.0.1:5579
Feb 13 09:03:48 host1 docker[5000]: time="2025-02-13T09:03:48Z" level=info msg="Network overlay joined"
Feb 13 09:03:50 host1 nginx[4001]: 10.0.0.12 - - [13/Feb/2025:09:03:50 +0000] "GET /api/metrics HTTP/1.1" 200 256
Feb 13 09:03:52 host1 postfix/qmgr[1200]: D4E5F6G7H8: removed
Feb 13 09:03:55 host1 systemd[1]: Started PackageKit Daemon
Feb 13 09:03:58 host1 app-backend[7001]: INFO: Database query completed in 8ms - SELECT COUNT(*) FROM orders
Feb 13 09:04:00 host1 mysqld[2100]: 2025-02-13T09:04:00.456789Z 0 [Note] Event Scheduler: Purging the queue. 0 events
Feb 13 09:04:02 host1 kernel: [12537.000] IPv6: ADDRCONF(NETDEV_UP): docker0: link is not ready
Feb 13 09:04:05 host1 apache2[4521]: AH00098: pid 4523: segfault at 0 ip 00007f1234567890 sp 00007fffabcdef00 error 4 in libc-2.31.so
Feb 13 09:04:08 host1 app-backend[7001]: CRITICAL: Certificate expiry in 7 days - hostname=api.example.com
Feb 13 09:04:10 host1 redis-server[3001]: 9:M 13 Feb 2025 09:04:10.456 * Defragmentation ended
Feb 13 09:04:12 host1 nginx[4001]: 10.0.0.13 - - [13/Feb/2025:09:04:12 +0000] "POST /api/logout HTTP/1.1" 200 32
FEED
```

Or append line-by-line, for example:

```bash
echo 'Feb 13 09:00:38 host1 apache2[4521]: ERROR: Apache failed to start. Port 80 is in use.' >> server.log
echo 'Feb 13 09:00:58 host1 app-backend[7001]: CRITICAL: Database connection timeout after 30 seconds' >> server.log
```

The monitor detects lines containing **ERROR**, **CRITICAL**, **Failed**, or **Timeout** and runs the full pipeline (LLM analysis, MCP, approval/dry-run).

---

## ðŸ“„ License

[Add your license information here]

## ðŸ‘¥ Authors

Zakariae Khmies, Mariam Chajia, Mohamed-yahia Ghounbaz, Nizar Abou-otmane
Faculty of Computer Science, WSM Warsaw, Poland
